{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import streamlit as st\n",
    "import pandas_datareader.data as web\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir o período para os dados que você deseja obter\n",
    "start = datetime(2000, 1, 1)\n",
    "end = datetime.now()\n",
    "\n",
    "# Obter os dados da taxa de juros do Fed (FEDFUNDS)\n",
    "df = web.DataReader('FEDFUNDS', 'fred', start, end)\n",
    "\n",
    "# Exibir os primeiros registros\n",
    "print(df.tail())\n",
    "\n",
    "# Exibir o gráfico da taxa de juros ao longo do tempo\n",
    "df.plot(title='Taxa de Juros do Fed ao longo do tempo')\n",
    "plt.xlabel('Ano')\n",
    "plt.ylabel('Taxa de Juros (%)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  GDP\n",
      "DATE                 \n",
      "2023-01-01  26813.601\n",
      "2023-04-01  27063.012\n",
      "2023-07-01  27610.128\n",
      "2023-10-01  27956.998\n",
      "2024-01-01  28255.928\n",
      "            CPIAUCSL\n",
      "DATE                \n",
      "2024-01-01   309.685\n",
      "2024-02-01   311.054\n",
      "2024-03-01   312.230\n",
      "2024-04-01   313.207\n",
      "2024-05-01   313.225\n"
     ]
    }
   ],
   "source": [
    "start = datetime(2000, 1, 1)\n",
    "end = datetime.now()\n",
    "\n",
    "# PIB dos EUA\n",
    "gdp = web.DataReader('GDP', 'fred', start, end)\n",
    "\n",
    "# Índice de Preços ao Consumidor\n",
    "cpi = web.DataReader('CPIAUCSL', 'fred', start, end)\n",
    "\n",
    "print(gdp.tail())\n",
    "print(cpi.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            UNRATE\n",
      "DATE              \n",
      "2024-01-01     3.7\n",
      "2024-02-01     3.9\n",
      "2024-03-01     3.8\n",
      "2024-04-01     3.9\n",
      "2024-05-01     4.0\n",
      "              INDPRO\n",
      "DATE                \n",
      "2024-01-01  101.7240\n",
      "2024-02-01  102.5863\n",
      "2024-03-01  102.4458\n",
      "2024-04-01  102.4547\n",
      "2024-05-01  103.3284\n",
      "              SP500\n",
      "DATE               \n",
      "2024-06-12  5421.03\n",
      "2024-06-13  5433.74\n",
      "2024-06-14  5431.60\n",
      "2024-06-17  5473.23\n",
      "2024-06-18  5487.03\n",
      "            T10YIE\n",
      "DATE              \n",
      "2024-06-12    2.22\n",
      "2024-06-13    2.21\n",
      "2024-06-14    2.17\n",
      "2024-06-17    2.22\n",
      "2024-06-18    2.21\n",
      "            DEXUSEU\n",
      "DATE               \n",
      "2024-06-10   1.0751\n",
      "2024-06-11   1.0733\n",
      "2024-06-12   1.0843\n",
      "2024-06-13   1.0756\n",
      "2024-06-14   1.0699\n",
      "               TOTALSL\n",
      "DATE                  \n",
      "2023-12-01  5023.69959\n",
      "2024-01-01  5036.14798\n",
      "2024-02-01  5047.96489\n",
      "2024-03-01  5046.86603\n",
      "2024-04-01  5053.26864\n",
      "            DCOILWTICO\n",
      "DATE                  \n",
      "2024-05-24       78.48\n",
      "2024-05-27         NaN\n",
      "2024-05-28       80.90\n",
      "2024-05-29       80.24\n",
      "2024-05-30       78.96\n",
      "              POPTHM\n",
      "DATE                \n",
      "2023-12-01  336070.0\n",
      "2024-01-01  336194.0\n",
      "2024-02-01  336306.0\n",
      "2024-03-01  336423.0\n",
      "2024-04-01  336550.0\n"
     ]
    }
   ],
   "source": [
    "start = datetime(2000, 1, 1)\n",
    "end = datetime.now()\n",
    "\n",
    "# Taxa de Desemprego\n",
    "unemployment_rate = web.DataReader('UNRATE', 'fred', start, end)\n",
    "\n",
    "# Índice de Produção Industrial\n",
    "industrial_production = web.DataReader('INDPRO', 'fred', start, end)\n",
    "\n",
    "# S&P 500\n",
    "sp500 = web.DataReader('SP500', 'fred', start, end)\n",
    "\n",
    "# Taxa de Inflação\n",
    "inflation_rate = web.DataReader('T10YIE', 'fred', start, end)\n",
    "\n",
    "# Taxa de Câmbio do Dólar\n",
    "exchange_rate = web.DataReader('DEXUSEU', 'fred', start, end)\n",
    "\n",
    "# Dívida dos Consumidores\n",
    "consumer_debt = web.DataReader('TOTALSL', 'fred', start, end)\n",
    "\n",
    "# Preço do Petróleo\n",
    "oil_price = web.DataReader('DCOILWTICO', 'fred', start, end)\n",
    "\n",
    "# População\n",
    "population = web.DataReader('POPTHM', 'fred', start, end)\n",
    "\n",
    "# Salários Médios\n",
    "average_wages = web.DataReader('CES0500000003', 'fred', start, end)\n",
    "\n",
    "# Taxa de Poupança dos Consumidores\n",
    "savings_rate = web.DataReader('PSAVERT', 'fred', start, end)\n",
    "\n",
    "# Confiança do Consumidor\n",
    "consumer_confidence = web.DataReader('UMCSENT', 'fred', start, end)\n",
    "\n",
    "# Vendas no Varejo\n",
    "retail_sales = web.DataReader('RSAFS', 'fred', start, end)\n",
    "\n",
    "print(\"\\nTaxa de Desemprego:\")\n",
    "print(unemployment_rate.tail())\n",
    "\n",
    "print(\"\\nÍndice de Produção Industrial:\")\n",
    "print(industrial_production.tail())\n",
    "\n",
    "print(\"\\nS&P 500:\")\n",
    "print(sp500.tail())\n",
    "\n",
    "print(\"\\nTaxa de Inflação:\")\n",
    "print(inflation_rate.tail())\n",
    "\n",
    "print(\"\\nTaxa de Câmbio do Dólar:\")\n",
    "print(exchange_rate.tail())\n",
    "\n",
    "print(\"\\nDívida dos Consumidores:\")\n",
    "print(consumer_debt.tail())\n",
    "\n",
    "print(\"\\nPreço do Petróleo:\")\n",
    "print(oil_price.tail())\n",
    "\n",
    "print(\"\\nPopulação:\")\n",
    "print(population.tail())\n",
    "\n",
    "print(\"\\nSalários Médios:\")\n",
    "print(average_wages.tail())\n",
    "\n",
    "print(\"\\nTaxa de Poupança dos Consumidores:\")\n",
    "print(savings_rate.tail())\n",
    "\n",
    "print(\"\\nConfiança do Consumidor:\")\n",
    "print(consumer_confidence.tail())\n",
    "\n",
    "print(\"\\nVendas no Varejo:\")\n",
    "print(retail_sales.tail())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                GPDI\n",
      "DATE                \n",
      "2023-01-01  4725.828\n",
      "2023-04-01  4780.290\n",
      "2023-07-01  4915.033\n",
      "2023-10-01  4954.426\n",
      "2024-01-01  5004.419\n"
     ]
    }
   ],
   "source": [
    "# dados de investimentos privados brutos\n",
    "investments = web.DataReader('GPDI', 'fred', start, end)\n",
    "print(investments.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Exportações  Importações\n",
      "DATE                                \n",
      "2023-01-01     3064.804     3890.491\n",
      "2023-04-01     2961.759     3767.851\n",
      "2023-07-01     3030.751     3809.982\n",
      "2023-10-01     3051.660     3835.394\n",
      "2024-01-01     3080.858     3930.952\n"
     ]
    }
   ],
   "source": [
    "import pandas_datareader.data as web\n",
    "from datetime import datetime\n",
    "\n",
    "# Defina o período de tempo\n",
    "start = datetime(2000, 1, 1)\n",
    "end = datetime.now()\n",
    "\n",
    "# Coletar dados de exportações e importações \n",
    "exports = web.DataReader('EXPGS', 'fred', start, end)\n",
    "imports = web.DataReader('IMPGS', 'fred', start, end)\n",
    "\n",
    "# Combine os dados em um DataFrame\n",
    "trade_data = pd.concat([exports, imports], axis=1)\n",
    "trade_data.columns = ['Exportações', 'Importações']\n",
    "\n",
    "print(trade_data.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            SAVINGS_BY_SOURCE\n",
      "DATE                         \n",
      "2020-03-30            10453.0\n",
      "2020-04-06            10435.3\n",
      "2020-04-13            10415.0\n",
      "2020-04-20            10625.8\n",
      "2020-04-27            10892.0\n"
     ]
    }
   ],
   "source": [
    "# Exemplo fictício para coletar dados de poupança\n",
    "savings = web.DataReader('SAVINGS_BY_SOURCE', 'fred', start, end)\n",
    "print(savings.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "tempos = ['1m', '5m', '15m', '30m', '1h', '2h', '4h', '8h', '1d', '3d', '1w']\n",
    "\n",
    "url = f\"https://api.binance.us/api/v3/klines?symbol={symbol}&interval={interval}&startTime={i}&endTime={i+86400000*5*ace}&limit={limit}\"\n",
    "\n",
    "requisicao_1w = requests.get(\"https://api.binance.us/api/v3/klines?symbol=BTCUSDT&interval=1w&limit=1000\")\n",
    "\n",
    "\n",
    "colunas = ['Timestamp', 'Open', 'High', 'Low', 'Close', 'Volume', 'CloseTime', 'QuoteAssetVolume', 'NumberOfTrades', 'TakerBuyBaseAssetVolume', 'TakerBuyQuoteAssetVolume', 'Ignore']\n",
    "\n",
    "dados = pd.DataFrame(dados_historicos, columns=colunas).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "response = requests.get(\"https://api.binance.us/api/v3/exchangeInfo\")\n",
    "\n",
    "if response.status_code == 200:\n",
    "    data = response.json()\n",
    "\n",
    "    for item in data[\"symbols\"]:\n",
    "        if item[\"symbol\"].endswith(\"USDT\"):\n",
    "            cripyto = item[\"symbol\"][:-4]\n",
    "            if cripyto not in carteira:\n",
    "                print(cripyto)\n",
    "else:\n",
    "    print('ERROR')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# API da Alternative.me\n",
    "def get_fear_and_greed_index():\n",
    "    response = requests.get('https://api.alternative.me/fng/')\n",
    "    try:\n",
    "        data = response.json()\n",
    "        return data['data'][0]['value'], data['data'][0]['value_classification']\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Erro ao decodificar JSON. Verifique a resposta da API.\")\n",
    "        return None, None\n",
    "\n",
    "# API da Alternative.me (Dominância do Bitcoin)\n",
    "def get_bitcoin_dominance():\n",
    "    response = requests.get('https://api.alternative.me/fng/dominance')  # Não Funciona\n",
    "    try:\n",
    "        data = response.json()\n",
    "        return data['data']['bitcoin_dominance']\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Erro ao decodificar JSON. Verifique a resposta da API.\")\n",
    "        return None\n",
    "\n",
    "# API da Paxful\n",
    "def get_paxful_bitcoin_dominance():\n",
    "    response = requests.get('https://api.paxful.com/v1/market-data/bitcoin-dominance')\n",
    "    try:\n",
    "        data = response.json()\n",
    "        if 'data' in data and 'bitcoin_dominance' in data['data']:\n",
    "            return data['data']['bitcoin_dominance']\n",
    "        else:\n",
    "            print(\"Erro ao decodificar JSON. Verifique a resposta da API.\")\n",
    "            return None\n",
    "    except json.JSONDecodeError:\n",
    "        print(\"Erro ao decodificar JSON. Verifique a resposta da API.\")\n",
    "        return None\n",
    "\n",
    "# Executar as APIs\n",
    "fear_and_greed_index, status = get_fear_and_greed_index()\n",
    "bitcoin_dominance = get_bitcoin_dominance()\n",
    "paxful_bitcoin_dominance = get_paxful_bitcoin_dominance()\n",
    "\n",
    "if fear_and_greed_index is not None and status is not None:\n",
    "    print(f\"Valor atual do Fear and Greed Index: {fear_and_greed_index}\")\n",
    "    print(f\"Status do Fear and Greed Index: {status}\")\n",
    "else:\n",
    "    print(\"Não foi possível obter o Fear and Greed Index.\")\n",
    "\n",
    "print(f\"Dominância do Bitcoin (Alternative.me): {bitcoin_dominance}\")\n",
    "print(f\"Dominância do Bitcoin (Paxful): {paxful_bitcoin_dominance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "\n",
    "#gets data from coingecko\n",
    "response = requests.get('https://api.coingecko.com/api/v3/coins/markets?vs_currency=usd&order=market_cap_desc&per_page=250&page=1&sparkline=false')\n",
    "response = response.json()\n",
    "\n",
    "#initialises variables\n",
    "BTCCap = 0\n",
    "altCap = 0\n",
    "current_time = datetime.now().strftime(\"%d-%m-%Y\")\n",
    "#current_time = datetime.now().strftime(\"%d-%m-%Y %H:%M:%S\")\n",
    "\n",
    "\n",
    "#iterates through response\n",
    "for x in response:\n",
    "    #print(x['id'])\n",
    "    if x['id'] == \"bitcoin\": #adds bitcoin market cap to BTCCap and altCap\n",
    "        BTCCap = x['market_cap']\n",
    "        altCap = altCap + x['market_cap']\n",
    "    else: #adds any altcoin market cap to altCap\n",
    "        altCap = altCap + x['market_cap']\n",
    "        \n",
    "with open(\"BTCDominance.csv\", mode='a+', newline =\"\") as datawriter: #writes the data to a CSV BTCDominance, will generate the file if it does not exist.\n",
    "    datawriter = csv.writer(datawriter, delimiter=\",\")\n",
    "    datawriter.writerow([current_time,\"{:.2f}\".format((BTCCap/altCap)*100)]) #writes bitcoin dominance out of 100 to 2dp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BTC\n",
    "# Fechamento\n",
    "# Media movel\n",
    "# Separar ano mes e dia\n",
    "#   df['ano'] = df['Date].dt.year\n",
    "#   df['mes'] = df['Date].dt.month\n",
    "#   df['dia'] = df['Date].dt.day\n",
    "# Rendimento (close(n) / Close(n-1)) -> dados['Close'] / dados['Close'].shift() - 1\n",
    "# Rentabilidade por mes -> df.groupby('mes').agg({'rentabilidade: sum'}).plot(kind='bar')  # Melhor mes para investir (menor valor)  # Testar outro alem do sum\n",
    "# Rentabilidade por mes -> df.groupby('dia').agg({'rentabilidade: sum'}).plot(kind='bar')  # Melhor dia para investir (menor valor)\n",
    "# Variância\n",
    "# RSI\n",
    "# Retorno frente ao BTC e frente ao IBOVE e SP500\n",
    "# ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import date\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas_ta as ta\n",
    "import yfinance\n",
    "plt.style.use('default')\n",
    "\n",
    "\n",
    "# DFA (Análise de Flutuação Detendida)\n",
    "# MFDFA (Análise de Flutuação com Tendência Multifractal)\n",
    "# DCCA (Análise de Correlação Cruzada Detendida)\n",
    "# MFDCCA (Análise de Correlação Cruzada Multifractal Detendida)\n",
    "# HT (expoente de Hurst dependente do tempo)\n",
    "\n",
    "# np.set_printoptions(precision=10)\n",
    "\n",
    "def calculate_rsi(data, window=14):\n",
    "    # Calcular as variações de preço\n",
    "    deltas = data['Open'].diff()\n",
    "\n",
    "    # Separar as variações de preço em positivas e negativas\n",
    "    gain = deltas.where(deltas > 0, 0)\n",
    "    loss = -deltas.where(deltas < 0, 0)\n",
    "\n",
    "    # Calcular as médias móveis\n",
    "    average_gain = gain.rolling(window=window, min_periods=1).mean()\n",
    "    average_loss = loss.rolling(window=window, min_periods=1).mean()\n",
    "\n",
    "    # Calcular o RSI\n",
    "    rs = average_gain / average_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "\n",
    "    return rsi\n",
    "\n",
    "\n",
    "def calculate_rvi(data, window=10):\n",
    "    close_prices_diff = data['Open'].diff()\n",
    "    upward_movements = close_prices_diff.where(close_prices_diff > 0, 0).rolling(window=window).mean()\n",
    "    downward_movements = close_prices_diff.where(close_prices_diff < 0, 0).abs().rolling(window=window).mean()\n",
    "\n",
    "    # Calcular o RVI\n",
    "    rvi = (upward_movements / (upward_movements + downward_movements)) * 100\n",
    "\n",
    "    return rvi\n",
    "\n",
    "\n",
    "def selectAnaliseTec(dados):\n",
    "    qtd_data = len(dados)\n",
    "    train_size = int(0.9 * qtd_data)\n",
    "    test_size = int(0.1 * qtd_data)\n",
    "\n",
    "    val_size = qtd_data - train_size - test_size\n",
    "\n",
    "    #separando as features e labels\n",
    "    features = dados.drop(['Close', 'High', 'Low', 'Dividends', 'Stock Splits'], axis=1)\n",
    "    labels1 = dados['Close']\n",
    "    labels2 = dados['High']\n",
    "    labels3 = dados['Low']\n",
    "\n",
    "    return qtd_data, train_size, test_size, val_size, features, labels1, labels2, labels3\n",
    "\n",
    "\n",
    "def hurst_dfa(data):\n",
    "    # Pré-processamento da série temporal\n",
    "    data = np.array(data)\n",
    "    data = data - np.mean(data)\n",
    "    data = data / np.std(data)\n",
    "    # print(np.std(data))\n",
    "\n",
    "    # Cálculo da função de flutuação\n",
    "    n_windows = len(data) // 2\n",
    "    ws = np.logspace(np.log10(2), np.log10(n_windows), n_windows)\n",
    "    F = np.zeros(n_windows)\n",
    "    for i, w in enumerate(ws):\n",
    "        n = int(w)\n",
    "        profile = np.cumsum(data[:n]) - np.mean(data[:n]) * np.arange(n)\n",
    "        F[i] = np.sqrt(np.mean((profile**2)[1:]))\n",
    "\n",
    "    # Ajuste da função de flutuação\n",
    "    H, H_intercept = np.polyfit(np.log10(ws), np.log10(F), 1)\n",
    "\n",
    "    return H, H_intercept\n",
    "\n",
    "\n",
    "def identify_fractals(data):\n",
    "    is_fractals = pd.Series(0, index=data.index, name='Fractal')\n",
    "\n",
    "    for i in range(2, len(data) - 2):\n",
    "        prev_high = data['High'].iloc[i-1]\n",
    "        next_high = data['High'].iloc[i+1]\n",
    "        next_next_high = data['High'].iloc[i+2]\n",
    "        prev_low = data['Low'].iloc[i-1]\n",
    "        next_low = data['Low'].iloc[i+1]\n",
    "        next_next_low = data['Low'].iloc[i+2]\n",
    "        prev_close = data['Close'].iloc[i-1]\n",
    "        curr_high = data['High'].iloc[i]\n",
    "        curr_low = data['Low'].iloc[i]\n",
    "        curr_close = data['Close'].iloc[i]\n",
    "\n",
    "        # Fractal de Alta\n",
    "        if (curr_high > prev_high and curr_high > next_high and curr_high > next_next_high and\n",
    "            curr_high == data['High'].iloc[i-2:i+1].max() and curr_close < prev_close):\n",
    "            is_fractals.iloc[i] = 1\n",
    "\n",
    "        # Fractal de Baixa\n",
    "        elif (curr_low < prev_low and curr_low < next_low and curr_low < next_next_low and\n",
    "              curr_low == data['Low'].iloc[i-2:i+1].min() and curr_close > prev_close):\n",
    "            is_fractals.iloc[i] = -1\n",
    "\n",
    "    data = data.join(is_fractals)\n",
    "    return data\n",
    "\n",
    "v_open = []\n",
    "pred_close = []\n",
    "pred_high = []\n",
    "pred_low = []\n",
    "coef_det_c = []\n",
    "coef_det_h = []\n",
    "coef_det_l = []\n",
    "cont = 0\n",
    "\n",
    "# Sem correlação entre os dados\n",
    "# UNI\n",
    "\n",
    "# Dados Insuficientes\n",
    "# PYTH, SEI, GRT, WIF, PEPE\n",
    "\n",
    "periodo = 0\n",
    "n = 0\n",
    "\n",
    "if periodo == 0:\n",
    "    preds = 25\n",
    "elif periodo == 1:\n",
    "    preds = 8\n",
    "elif periodo == 2:\n",
    "    preds = 169\n",
    "elif periodo == 3:\n",
    "    preds = 61\n",
    "elif periodo == 4:\n",
    "    preds = 49\n",
    "elif periodo == 5:\n",
    "    preds = 31\n",
    "\n",
    "ativos = ['BTC', 'ETH', 'MATIC', 'INJ', 'NEAR', 'MANA', 'RNDR', 'ADA', 'LINK',\n",
    "          'FET', 'GALA', 'VET', 'MKR', 'FIL', 'SOL', 'DOT', 'AGIX', 'AVAX',\n",
    "          'PENDLE', 'THETA', 'SHIB', 'TON', 'OP', 'BNB', 'ICP']\n",
    "\n",
    "for i in range(1, preds+1):\n",
    "\n",
    "    symbol = f'{ativos[n]}-USD'\n",
    "    ticker = yfinance.Ticker(symbol)\n",
    "    if periodo == 1 or periodo == 3 or periodo == 5:\n",
    "        dados = ticker.history(period='max', interval='1d')\n",
    "    elif periodo == 0 or periodo == 2 or periodo == 4:\n",
    "        dados = ticker.history(period='720d', interval='1h')\n",
    "    \n",
    "    dados['mm9d'] = dados['Close'].rolling(9).mean()\n",
    "    dados['mm14d'] = dados['Close'].rolling(14).mean()\n",
    "    dados['mm21d'] = dados['Close'].rolling(21).mean()\n",
    "    dados['mm12d'] = dados['Close'].rolling(12).mean()\n",
    "    dados['mm26d'] = dados['Close'].rolling(26).mean()\n",
    "    dados['MACD_linha'] = dados['mm12d'] - dados['mm26d']\n",
    "    dados['MACD_media'] = dados['MACD_linha'].rolling(9).mean()\n",
    "    dados['MACD_hist'] = dados['MACD_linha'] - dados['MACD_media']\n",
    "    dados['RSI'] = calculate_rsi(dados)\n",
    "    dados['RVI'] = calculate_rvi(dados)\n",
    "    dados['mme17d'] = dados['Close'].ewm(span=17, adjust=False).mean()\n",
    "    dados['est_RSI'] = (dados['RSI'] - dados['RSI'].rolling(14).min()) / (dados['RSI'].rolling(14).max() - dados['RSI'].rolling(14).min())\n",
    "    dados['est_RSI_D'] = dados['est_RSI'].rolling(3).mean()\n",
    "    bb_plot = dados.ta.bbands(close='Open', length=20, std=2)\n",
    "    dados = pd.concat([dados, bb_plot], axis=1)\n",
    "    dados['LogReturn'] = np.log(dados['Close'] / dados['Close'].shift(1))\n",
    "    dados['EWMA_LogReturn'] = dados['LogReturn'].ewm(span=14, adjust=False).mean()\n",
    "    dados['Volatility'] = dados['LogReturn'].rolling(14).std()\n",
    "\n",
    "    # dados.to_csv(f'{symbol}-{interval}.csv', index=False)\n",
    "\n",
    "    dados['mm55d'] = dados['Close'].rolling(55).mean()\n",
    "    dados['mme55d'] = dados['Close'].ewm(span=55, adjust=False).mean()\n",
    "\n",
    "    if len(dados) > 1500:\n",
    "        dados['mm350d'] = dados['Close'].rolling(350).mean()*2\n",
    "        dados['mme200d'] = dados['Close'].ewm(span=200, adjust=False).mean()\n",
    "        dados['mm111d'] = dados['Close'].rolling(111).mean()\n",
    "\n",
    "        dados['FRAC'] = identify_fractals(dados.loc[:,['Open', 'High', 'Low', 'Close']])['Fractal']\n",
    "\n",
    "        dados['Close'] = dados['Close'].shift(-i)\n",
    "        dados['High'] = dados['High'].shift(-i)\n",
    "        dados['Low'] = dados['Low'].shift(-i)\n",
    "\n",
    "        dur = 30\n",
    "        H_list = []\n",
    "        H_intercept_list = []\n",
    "        for i in range(0, len(dados) - dur):\n",
    "            if i < dur:\n",
    "                H_list.append(None)\n",
    "                H_intercept_list.append(None)\n",
    "\n",
    "            tam = i + dur\n",
    "            H, H_intercept = hurst_dfa(dados['Open'][i:tam])\n",
    "\n",
    "            H_list.append(H)\n",
    "            H_intercept_list.append(H_intercept)\n",
    "\n",
    "        dados.insert(27, \"H\", H_list)\n",
    "        dados.insert(28, \"H_intercept\", H_intercept_list)\n",
    "\n",
    "    else:\n",
    "        dados['Close'] = dados['Close'].shift(-i)\n",
    "        dados['High'] = dados['High'].shift(-i)\n",
    "        dados['Low'] = dados['Low'].shift(-i)\n",
    "    \n",
    "    dados.dropna(inplace=True)\n",
    "    qtd_data, train_size, test_size, val_size, features, labels1, labels2, labels3 = selectAnaliseTec(dados)\n",
    "\n",
    "    if len(dados) > 1500:\n",
    "        features = dados.loc[:,['Open', 'mm9d', 'mm14d', 'mm21d', 'mm55d', 'mm12d',\n",
    "                            'mm26d', 'MACD_linha', 'MACD_media', 'MACD_hist',\n",
    "                            'RVI', 'RSI', 'mm111d', 'mm350d', 'mme17d', 'mme55d',\n",
    "                            'mme200d', 'H', 'H_intercept', 'est_RSI', 'est_RSI_D',\n",
    "                            'BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0', 'BBB_20_2.0',\n",
    "                            'BBP_20_2.0', 'FRAC', 'LogReturn', 'EWMA_LogReturn',\n",
    "                            'Volatility', 'Volume']]\n",
    "    else:\n",
    "        features = dados.loc[:,['Open', 'mm9d', 'mm14d', 'mm21d', 'mm55d', 'mm12d',\n",
    "                            'mm26d', 'MACD_linha', 'MACD_media', 'MACD_hist', 'RVI',\n",
    "                            'RSI', 'mme17d', 'mme55d', 'est_RSI', 'est_RSI_D',\n",
    "                            'BBL_20_2.0', 'BBM_20_2.0', 'BBU_20_2.0', 'BBB_20_2.0',\n",
    "                            'BBP_20_2.0', 'LogReturn', 'EWMA_LogReturn', 'Volatility',\n",
    "                            'Volume']]\n",
    "\n",
    "    #Separa os dados de treino teste e validação\n",
    "    X_train = features[:train_size]\n",
    "    X_test = features[train_size:train_size + test_size -1]\n",
    "\n",
    "    y_train1 = labels1[:train_size]\n",
    "    y_test1 = labels1[train_size:train_size + test_size -1]\n",
    "\n",
    "    y_train2 = labels2[:train_size]\n",
    "    y_test2 = labels2[train_size:train_size + test_size -1]\n",
    "\n",
    "    y_train3 = labels3[:train_size]\n",
    "    y_test3 = labels3[train_size:train_size + test_size -1]\n",
    "\n",
    "    # Normalizando os dados de entrada(features)\n",
    "\n",
    "    # Gerando o novo padrão\n",
    "    scaler = MinMaxScaler()\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scale = scaler.fit_transform(X_train)  # Normalizando os dados de entrada(treinamento)\n",
    "    X_test_scale  = scaler.transform(X_test)       # Normalizando os dados de entrada(teste)\n",
    "\n",
    "    valor_novo = features.tail(1)\n",
    "    previsao = scaler.transform(valor_novo)\n",
    "\n",
    "\n",
    "    # treinamento usando regressão linear close\n",
    "\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(X_train_scale, y_train1)\n",
    "    pred = lr.predict(X_test_scale)\n",
    "    cd_close = r2_score(y_test1, pred)\n",
    "\n",
    "    #executando a previsão\n",
    "\n",
    "    pred_final = lr.predict(previsao)\n",
    "    pred_close.append(pred_final[0])\n",
    "\n",
    "    # treinamento usando regressão linear high\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(X_train_scale, y_train2)\n",
    "    pred = lr.predict(X_test_scale)\n",
    "    cd_high = r2_score(y_test2, pred)\n",
    "\n",
    "    #executando a previsão\n",
    "\n",
    "    pred_final = lr.predict(previsao)\n",
    "    pred_high.append(pred_final[0])\n",
    "\n",
    "    # treinamento usando regressão linear low\n",
    "    lr = linear_model.LinearRegression()\n",
    "    lr.fit(X_train_scale, y_train3)\n",
    "    pred = lr.predict(X_test_scale)\n",
    "    cd_low = r2_score(y_test3, pred)\n",
    "\n",
    "    #executando a previsão\n",
    "\n",
    "    pred_final = lr.predict(previsao)\n",
    "    pred_low.append(pred_final[0])\n",
    "\n",
    "\n",
    "    coef_det_c.append(f'{cd_close*100:.2f}')\n",
    "    coef_det_h.append(f'{cd_high*100:.2f}')\n",
    "    coef_det_l.append(f'{cd_low*100:.2f}')\n",
    "\n",
    "    if cont == 0:\n",
    "        v_open.append(dados['Close'].tail(1).values[0])\n",
    "    else:\n",
    "        v_open.append(pred_close[cont-1])\n",
    "\n",
    "    cont += 1\n",
    "\n",
    "# Plotar os valores\n",
    "coef_det_c_float = [float(x) for x in coef_det_c]\n",
    "coef_det_h_float = [float(x) for x in coef_det_h]\n",
    "coef_det_l_float = [float(x) for x in coef_det_l]\n",
    "\n",
    "# Criando uma figura com 2 subplots lado a lado\n",
    "fig, axs = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Plotando os valores de previsão no primeiro subplot\n",
    "axs[0].plot(pred_close, marker='o', linestyle='-', label='PrevS Close')\n",
    "axs[0].plot(pred_high, marker='s', linestyle='--', label='PrevS High')\n",
    "axs[0].plot(pred_low, marker='^', linestyle=':', label='PrevS Low')\n",
    "axs[0].set_xlabel('Índice')\n",
    "axs[0].set_ylabel('Valores')\n",
    "axs[0].set_title(f'Gráfico de PrevS {symbol} - 1d')\n",
    "axs[0].grid(True)\n",
    "axs[0].legend()\n",
    "\n",
    "# Plotando 'coef_det' no segundo subplot\n",
    "axs[1].plot(coef_det_c_float, marker='o', linestyle='-', label='Coeficiente de Determinação Close')\n",
    "axs[1].plot(coef_det_h_float, marker='s', linestyle='--', label='Coeficiente de Determinação High')\n",
    "axs[1].plot(coef_det_l_float, marker='^', linestyle=':', label='Coeficiente de Determinação Low')\n",
    "axs[1].set_xlabel('Índice')\n",
    "axs[1].set_ylabel('Coeficiente de Determinação (%)')\n",
    "axs[1].set_title('Coeficiente de Determinação')\n",
    "axs[1].grid(True)\n",
    "axs[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "# Mostrando a figura com os subplots\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "lista = np.array([2, 6, 4, 3, 5, 1, 8, 7, 9])\n",
    "np.median(lista)\n",
    "np.average(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
